{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from FairRanking.helpers import nDCG_cls, disparate_impact, calc_accuracy, calc_sens_loss, rND_torch, auc_estimator, group_pairwise_accuracy\n",
    "from FairRanking.datasets.adult import Adult\n",
    "from FairRanking.datasets.law import Law\n",
    "from FairRanking.datasets.compas import Compas\n",
    "from FairRanking.datasets.wiki import Wiki\n",
    "from FairRanking.models.BaseDirectRanker import convert_data_to_tensors\n",
    "from FairRanking.models.DirectRankerAdv import DirectRankerAdv\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from FairRanking.writer import Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = Law('race')\n",
    "data = Adult()\n",
    "#data = Compas()\n",
    "#data = Wiki()\n",
    "(X_train, s_train, y_train), (X_val, s_val, y_val), (X_test, s_test, y_test) = data.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robert/Desktop/Bachelor/FairRanker/FairRanking/models/BaseDirectRanker.py:205: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_9d63z49rj_/croot/pytorch_1681837279022/work/torch/csrc/utils/tensor_new.cpp:233.)\n",
      "  x0 = torch.tensor(x0, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "X_train0, X_train1, s_train0, s_train1, y_train, X_val0, X_val1, s_val0, s_val1, y_val, X_test0, X_test1, s_test0, s_test1, y_test = convert_data_to_tensors(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "def calc_rnd(model, X0, X1, s0, s1):\n",
    "    zero_documents = torch.zeros(size=(X0.shape[0]+X1.shape[0], X0.shape[1]))\n",
    "    X_test_combined = torch.cat((X0, X1), dim=0)\n",
    "    shuffled = X_test_combined[torch.randperm(X_test_combined.size(0))]\n",
    "    s_test_combined = torch.cat((s0, s1), dim=0)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test_combined, shuffled)\n",
    "        s_test_combined = torch.argmax(s_test_combined, dim=1)\n",
    "        return rND_torch(predictions, s_test_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_phase(model, X_train0, X_train1, y_train, optimizer, loss_fn):\n",
    "    y_pred_train = model(X_train0, X_train1)\n",
    "    main_loss = loss_fn(y_train, y_pred_train)\n",
    "    main_loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return main_loss, y_pred_train\n",
    "\n",
    "\n",
    "def second_phase(model, X_train0, X_train1, s_train0, s_train1, optimizer, loss_fn, loss_threshold):\n",
    "    sensitive_pred0, sensitive_pred1 = model.forward_2(X_train0, X_train1)\n",
    "    sensitive_loss = loss_fn(torch.cat((sensitive_pred0, sensitive_pred1), dim=0), torch.cat((s_train0, s_train1), dim=0))\n",
    "    #sensitive_loss = loss_fn(sensitive_pred0, s_train0)\n",
    "    sensitive_loss.backward()\n",
    "    #reversed_gradients = [p.grad * -1.0 for p in model.parameters() if p.grad is not None]\n",
    "    for p in model.named_parameters():\n",
    "        if 'debias' in p[0]:\n",
    "            continue\n",
    "        else:\n",
    "            if p[1].grad is not None:\n",
    "                if sensitive_loss.item() < loss_threshold:\n",
    "                    p[1].grad *= -1\n",
    "                else:\n",
    "                    p[1].grad = torch.zeros_like(p[1].grad, dtype=torch.float32)\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return sensitive_loss\n",
    "\n",
    "def model_rel_evaluation(model, writer, epoch, loss_fn):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_train_pred = model(X_train0, X_train1)\n",
    "        y_val_pred = model(X_val0, X_val1)\n",
    "        di_train_score = disparate_impact(y_train_pred, s_train0, s_train1)\n",
    "        train_acc = calc_accuracy(y_pred_train, y_train)\n",
    "        val_acc = calc_accuracy(y_val_pred, y_val)\n",
    "        di_val_score = disparate_impact(y_val_pred, s_val0, s_val1)\n",
    "        auc_train = auc_estimator(y_pred_train, y_train)\n",
    "        auc_val = auc_estimator(y_val_pred, y_val)\n",
    "        rnd_train = calc_rnd(model, X_train0, X_train1, s_train0, s_train1)\n",
    "        rnd_val = calc_rnd(model, X_val0, X_val1, s_val0, s_val1)\n",
    "        ndcg_train = nDCG_cls(model, X_train0, X_train1, y_train, esti=False)\n",
    "        ndcg_val = nDCG_cls(model, X_val0, X_val1, y_val, esti=False)\n",
    "        gpa_train = group_pairwise_accuracy(y_pred_train, y_train, s_train0)\n",
    "        gpa_val = group_pairwise_accuracy(y_val_pred, y_val, s_val0) \n",
    "        disparate_impact_losses = {'train_di': di_train_score, 'val_di': di_val_score}\n",
    "        epoch_accuracies = {'train_acc': train_acc, 'val_acc': val_acc}\n",
    "        epoch_auc = {'train_auc': auc_train, 'val_auc': auc_val}\n",
    "        epoch_rnd = {'train_rnd': rnd_train, 'val_rnd': rnd_val}\n",
    "        epoch_ndcg = {'train_ndcg': ndcg_train, 'val_ndcg': ndcg_val}\n",
    "        epoch_gpa = {'train_gpa': gpa_train, 'val_ndcg': gpa_val}\n",
    "        input = {\n",
    "            'Accuracy': [epoch_accuracies, epoch],\n",
    "            'Disparate Impact': [disparate_impact_losses, epoch],\n",
    "            'AUC': [epoch_auc, epoch],\n",
    "            'rND': [epoch_rnd, epoch],\n",
    "            'NDCG': [epoch_ndcg, epoch],\n",
    "            'GPA': [epoch_gpa, epoch]\n",
    "            }\n",
    "        return train_acc, di_train_score, val_acc, di_val_score\n",
    "\n",
    "def model_evaluation(model, sensitive_loss, writer, writer_epoch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_train_pred = model(X_train0, X_train1)\n",
    "        y_val_pred = model(X_val0, X_val1)\n",
    "        y_pred_train = model(X_train0, X_train1)\n",
    "        di_train_score = disparate_impact(y_pred_train, s_train0, s_train1)\n",
    "        di_val_score = disparate_impact(y_val_pred, s_val0, s_val1)\n",
    "        sensitive_val_pred0, sensitive_val_pred1 = model.forward_2(X_val0, X_val1)\n",
    "        sensitive_val_loss = calc_sens_loss(sensitive_val_pred0, sensitive_val_pred1, s_val0, s_val1, gamma=1.0)\n",
    "        train_acc = calc_accuracy(y_pred_train, y_train)\n",
    "        val_acc = calc_accuracy(y_val_pred, y_val)\n",
    "        auc_train = auc_estimator(y_pred_train, y_train)\n",
    "        auc_val = auc_estimator(y_val_pred, y_val)\n",
    "        rnd_train = calc_rnd(model, X_train0, X_train1, s_train0, s_train1)\n",
    "        rnd_val = calc_rnd(model, X_val0, X_val1, s_val0, s_val1)\n",
    "        ndcg_train = nDCG_cls(model, X_train0, X_train1, y_train, esti=False)\n",
    "        ndcg_val = nDCG_cls(model, X_val0, X_val1, y_val, esti=False)\n",
    "        gpa_train = group_pairwise_accuracy(y_pred_train, y_train, s_train0)\n",
    "        gpa_val = group_pairwise_accuracy(y_val_pred, y_val, s_val0) \n",
    "        sensitive_losses = {'train_sensitive': sensitive_loss, 'val_sensitive': sensitive_val_loss}\n",
    "        disparate_impact_losses = {'train_di': di_train_score, 'val_di': di_val_score}\n",
    "        epoch_accuracies = {'train_acc': train_acc, 'val_acc': val_acc}\n",
    "        epoch_auc = {'train_auc': auc_train, 'val_auc': auc_val}\n",
    "        epoch_rnd = {'train_rnd': rnd_train, 'val_rnd': rnd_val}\n",
    "        epoch_ndcg = {'train_ndcg': ndcg_train, 'val_ndcg': ndcg_val}\n",
    "        epoch_gpa = {'train_gpa': gpa_train, 'val_ndcg': gpa_val}\n",
    "        input = {\n",
    "            'Accuracy': [epoch_accuracies, epoch],\n",
    "            'Sensitive Loss': [sensitive_losses, epoch],\n",
    "            'Disparate Impact': [disparate_impact_losses, epoch],\n",
    "            'AUC': [epoch_auc, epoch],\n",
    "            'rND': [epoch_rnd, epoch],\n",
    "            'NDCG': [epoch_ndcg, epoch],\n",
    "            'GPA': [epoch_gpa, epoch]\n",
    "            }\n",
    "        writer.write(**input)\n",
    "        return train_acc, di_train_score, val_acc, sensitive_val_loss, di_val_score\n",
    "\n",
    "\n",
    "def only_adversarial(model, X_train0, X_train1, s_train0, s_train1, optimizer, loss_fn):\n",
    "    sensitive_pred0, sensitive_pred1 = model.forward_2(X_train0, X_train1)\n",
    "    sensitive_loss = loss_fn(torch.cat((sensitive_pred0, sensitive_pred1), dim=0), torch.cat((s_train0, s_train1), dim=0))\n",
    "    sensitive_loss.backward()\n",
    "    for p in model.named_parameters():\n",
    "        if 'debias' in p[0]:\n",
    "            #print(p[1])\n",
    "            continue\n",
    "        else:\n",
    "            if p[1].grad is not None:\n",
    "                p[1].grad = torch.zeros_like(p[1].grad, dtype=torch.float32)\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return sensitive_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4599\t Test Accuracy: 0.8425\t DI: 16.5048\n",
      "Finished\n",
      "Test Loss: 0.4139\t Test Accuracy: 0.8625\t DI: 18.9616\n",
      "Finished\n",
      "Test Loss: 0.4138\t Test Accuracy: 0.8628\t DI: 18.9616\n",
      "Finished\n",
      "Test Loss: 0.4139\t Test Accuracy: 0.8625\t DI: 18.9616\n",
      "Finished\n",
      "Test Loss: 0.4139\t Test Accuracy: 0.8625\t DI: 18.9616\n",
      "Finished\n",
      "Test Loss: 0.4135\t Test Accuracy: 0.8631\t DI: 18.9616\n",
      "Finished\n",
      "Test Loss: 0.4133\t Test Accuracy: 0.8631\t DI: 18.9616\n",
      "Finished\n",
      "Test Loss: 0.4497\t Test Accuracy: 0.8488\t DI: 18.5015\n",
      "Finished\n",
      "Test Loss: 0.4375\t Test Accuracy: 0.8485\t DI: 17.8231\n",
      "Finished\n",
      "Test Loss: 0.4138\t Test Accuracy: 0.8628\t DI: 18.9616\n",
      "Finished\n",
      "Test Loss: 0.4139\t Test Accuracy: 0.8625\t DI: 18.9616\n",
      "Finished\n",
      "Test Loss: 0.4139\t Test Accuracy: 0.8625\t DI: 18.9616\n",
      "Finished\n",
      "Test Loss: 0.4135\t Test Accuracy: 0.8631\t DI: 18.9616\n",
      "Finished\n",
      "Test Loss: 0.4133\t Test Accuracy: 0.8631\t DI: 18.9616\n",
      "Finished\n",
      "Test Loss: 0.4059\t Test Accuracy: 0.8609\t DI: 17.3195\n",
      "Finished\n",
      "Test Loss: 0.4139\t Test Accuracy: 0.8625\t DI: 18.9616\n",
      "Finished\n",
      "Test Loss: 0.4347\t Test Accuracy: 0.8517\t DI: 16.8397\n",
      "Finished\n",
      "Test Loss: 0.4139\t Test Accuracy: 0.8625\t DI: 18.9616\n",
      "Finished\n",
      "Test Loss: 0.4139\t Test Accuracy: 0.8625\t DI: 18.9616\n",
      "Finished\n",
      "Test Loss: 0.4135\t Test Accuracy: 0.8631\t DI: 18.9616\n",
      "Finished\n",
      "Test Loss: 0.4133\t Test Accuracy: 0.8631\t DI: 18.9616\n",
      "Finished\n",
      "Test Loss: 0.4262\t Test Accuracy: 0.8523\t DI: 17.9648\n",
      "Finished\n",
      "Test Loss: 0.4191\t Test Accuracy: 0.8609\t DI: 17.8701\n",
      "Finished\n",
      "Test Loss: 0.4162\t Test Accuracy: 0.8593\t DI: 18.0604\n",
      "Finished\n",
      "Test Loss: 0.4271\t Test Accuracy: 0.8498\t DI: 17.3195\n",
      "Finished\n",
      "Test Loss: 0.4139\t Test Accuracy: 0.8625\t DI: 18.9616\n",
      "Finished\n",
      "Test Loss: 0.4135\t Test Accuracy: 0.8631\t DI: 18.9616\n",
      "Finished\n",
      "Test Loss: 0.4133\t Test Accuracy: 0.8631\t DI: 18.9616\n",
      "Finished\n",
      "Test Loss: 0.4162\t Test Accuracy: 0.8549\t DI: 17.3642\n",
      "Finished\n",
      "Test Loss: 0.4513\t Test Accuracy: 0.8447\t DI: 17.2749\n",
      "Finished\n",
      "Test Loss: 0.4267\t Test Accuracy: 0.8501\t DI: 18.1568\n",
      "Finished\n",
      "Test Loss: 0.4139\t Test Accuracy: 0.8625\t DI: 18.9616\n",
      "Finished\n",
      "Test Loss: 0.4139\t Test Accuracy: 0.8625\t DI: 18.9616\n",
      "Finished\n",
      "Test Loss: 0.4135\t Test Accuracy: 0.8631\t DI: 18.9616\n",
      "Finished\n",
      "Test Loss: 0.4029\t Test Accuracy: 0.8584\t DI: 18.5015\n",
      "Finished\n",
      "Test Loss: 0.4112\t Test Accuracy: 0.8609\t DI: 17.5453\n",
      "Finished\n",
      "Test Loss: 0.4211\t Test Accuracy: 0.8479\t DI: 18.6527\n",
      "Finished\n",
      "Test Loss: 0.4099\t Test Accuracy: 0.8565\t DI: 17.5911\n",
      "Finished\n",
      "Test Loss: 0.4048\t Test Accuracy: 0.8584\t DI: 17.6833\n",
      "Finished\n",
      "Test Loss: 0.4139\t Test Accuracy: 0.8625\t DI: 18.9616\n",
      "Finished\n",
      "Test Loss: 0.4135\t Test Accuracy: 0.8631\t DI: 18.9616\n",
      "Finished\n",
      "Test Loss: 0.4407\t Test Accuracy: 0.8517\t DI: 17.4544\n",
      "Finished\n",
      "Test Loss: 0.4095\t Test Accuracy: 0.8603\t DI: 18.7036\n",
      "Finished\n",
      "Test Loss: 0.4191\t Test Accuracy: 0.8568\t DI: 17.8701\n",
      "Finished\n",
      "Test Loss: 0.4138\t Test Accuracy: 0.8628\t DI: 18.9616\n",
      "Finished\n",
      "Test Loss: 0.4330\t Test Accuracy: 0.8536\t DI: 16.9683\n",
      "Finished\n",
      "Test Loss: 0.4139\t Test Accuracy: 0.8625\t DI: 18.9616\n",
      "Finished\n",
      "Test Loss: 0.4500\t Test Accuracy: 0.8434\t DI: 16.9252\n",
      "Finished\n",
      "Test Loss: 0.5536\t Test Accuracy: 0.8017\t DI: 15.0930\n",
      "Finished\n",
      "ALL FINISHED!\n"
     ]
    }
   ],
   "source": [
    "scheduler_run = 'Schedule01'\n",
    "\n",
    "# rnd score in the original data\n",
    "y_test_full = torch.cat((y_test, (-1)*y_test), dim=0)\n",
    "s_test_full = torch.cat((s_test0, s_test1), dim=0)\n",
    "base_rnd = rND_torch(y_test_full, torch.argmax(s_test_full, dim=1))\n",
    "\n",
    "hidden_layers = [64, 32, 16]\n",
    "debias_layers = [128, 64, 32, 16]\n",
    "threshold = 0.35\n",
    "main_lr = 0.001\n",
    "adv_lr = 0.1\n",
    "k = 1\n",
    "schedules = [[i,j] for j in range(1,8) for i in range(1,8)]\n",
    "for schedule in schedules:\n",
    "    torch.manual_seed(42)\n",
    "    model = DirectRankerAdv(num_features=X_train0.shape[1],\n",
    "                     kernel_initializer=nn.init.normal_,\n",
    "                     hidden_layers=hidden_layers,\n",
    "                     bias_layers=debias_layers,\n",
    "             )\n",
    "    n_epochs = int(1000 / schedule[0])\n",
    "    \"\"\"writer = Writer(hidden_layers,\n",
    "                    schedule,\n",
    "                    scheduler_run,\n",
    "                    'adult',\n",
    "                    f'run_{k}',\n",
    "                    'directRankerADV',\n",
    "                    'disparateImpactNormalWeightInitial'\n",
    "                    )\n",
    "    writer.writer.add_text(tag='Debias Layers', text_string=str(debias_layers))\n",
    "    writer.writer.add_text(tag='Main lr', text_string=str(main_lr))\n",
    "    writer.writer.add_text(tag='Adv lr', text_string=str(adv_lr))\n",
    "    writer.writer.add_text(tag='threshold', text_string=str(threshold))\"\"\"\n",
    "    lr_decay = 0.944\n",
    "    optimizer = optim.Adam(model.parameters(), lr=main_lr)\n",
    "    adv_optimizer = optim.Adam(model.parameters(), lr=adv_lr)\n",
    "    opt_scheduler = StepLR(optimizer, step_size=500, gamma=lr_decay)\n",
    "    adv_scheduler = StepLR(adv_optimizer, step_size=500, gamma=lr_decay)\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "    sensitive_loss_fn = nn.BCEWithLogitsLoss()\n",
    "    writer_epoch = 0 \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for i in range(schedule[0]):\n",
    "            main_loss, y_pred_train = first_phase(model, X_train0, X_train1, y_train, optimizer, loss_fn)\n",
    "            model.eval()\n",
    "            train_loss = main_loss.item()\n",
    "            #train_acc, di_train_score, val_acc, di_val_score = \\\n",
    "            #    model_rel_evaluation(model, writer, epoch, loss_fn)\n",
    "            model.train()\n",
    "            writer_epoch += 1\n",
    "        for i in range(schedule[1]):\n",
    "            sensitive_loss = second_phase(model, X_train0, X_train1, s_train0, s_train1, adv_optimizer, sensitive_loss_fn, threshold)\n",
    "            #with open('sensitive_loss.txt', 'a') as file:\n",
    "            #    file.write(f\"{writer_epoch}    {sensitive_loss.item():.4f}\\n\")\n",
    "            model.eval()\n",
    "            #train_acc, di_train_score, val_acc, sensitive_val_loss, di_val_score = \\\n",
    "            #    model_evaluation(model, sensitive_loss, writer, writer_epoch)\n",
    "            writer_epoch += 1\n",
    "            model.train()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_test_pred = model(X_test0, X_test1)\n",
    "        test_loss = loss_fn(y_test, y_test_pred)\n",
    "        test_acc = calc_accuracy(y_test_pred, y_test)\n",
    "        di_test_score = disparate_impact(y_test_pred, s_test0, s_test1)\n",
    "        print(f'Test Loss: {test_loss.item():.4f}\\t Test Accuracy: {test_acc.item():.4f}\\t DI: {di_test_score:.4f}')\n",
    "        auc_test = auc_estimator(y_test_pred, y_test)\n",
    "        ndcg_test = nDCG_cls(model, X_test0, X_test1, y_test, esti=False)\n",
    "        gpa_test = group_pairwise_accuracy(y_test_pred, y_test, s_test0)\n",
    "        sensitive_pred0, sensitive_pred1 = model.forward_2(X_test0, X_test1)\n",
    "        sensitive_loss = sensitive_loss_fn(torch.cat((sensitive_pred0, sensitive_pred1), dim=0), torch.cat((s_test0, s_test1), dim=0))\n",
    "        rnd_arr = []\n",
    "        for i in range(100):\n",
    "            rnd_arr.append(calc_rnd(model, X_test0, X_test1, s_test0, s_test1))\n",
    "        rnd_test = np.mean(rnd_arr)\n",
    "        PATH = f'Results/{model.name}/{data.name}/{scheduler_run}/'\n",
    "        with open(f'{PATH}results.txt', 'a+') as file:\n",
    "            file.write(f'{auc_test},{test_acc.item()},{rnd_test},{ndcg_test},{gpa_test},{sensitive_loss},{str(schedule).replace(\",\",\";\")}\\n')\n",
    "    writer_epoch = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        for i in range(schedule[1]):\n",
    "            model.train()\n",
    "            sensitive_loss = only_adversarial(model, X_train0, X_train1, s_train0, s_train1, adv_optimizer, sensitive_loss_fn)\n",
    "            #writer.write({'Train Sensitive Loss': sensitive_loss})\n",
    "            #writer_epoch += 1\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sensitive_pred0, sensitive_pred1 = model.forward_2(X_test0, X_test1)\n",
    "        test_sensitive_loss = sensitive_loss_fn(torch.cat((sensitive_pred0, sensitive_pred1), dim=0), torch.cat((s_test0, s_test1), dim=0))\n",
    "        #writer.writer.add_text(tag='Sensitive Loss (Seperate Training)', text_string=str(test_sensitive_loss))\n",
    "        with open(f'{PATH}results_extra_sensitive_training.txt', 'a+') as file:\n",
    "            file.write(f'{str(str(schedule).replace(\",\",\";\"))},{test_sensitive_loss}\\n')\n",
    "    #del writer\n",
    "    k += 1\n",
    "    print('Finished')\n",
    "print(\"ALL FINISHED!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = DirectRankerAdv(num_features=X_train0.shape[1],\n",
    "                    kernel_initializer=nn.init.normal_,\n",
    "                    hidden_layers=hidden_layers,\n",
    "                    bias_layers=debias_layers,\n",
    "            )\n",
    "lr_decay = 0.944\n",
    "optimizer = optim.Adam(model.parameters(), lr=main_lr)\n",
    "adv_optimizer = optim.Adam(model.parameters(), lr=adv_lr)\n",
    "opt_scheduler = StepLR(optimizer, step_size=500, gamma=lr_decay)\n",
    "adv_scheduler = StepLR(adv_optimizer, step_size=500, gamma=lr_decay)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "sensitive_loss_fn = nn.BCEWithLogitsLoss()\n",
    "writer_epoch = 0 \n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    # Main\n",
    "    main_loss, y_pred_train = first_phase(model, X_train0, X_train1, y_train, optimizer, loss_fn)\n",
    "    model.eval()\n",
    "    train_loss = main_loss.item()\n",
    "    model.train()\n",
    "    writer_epoch += 1\n",
    "    # Adversarial\n",
    "    sensitive_loss = second_phase(model, X_train0, X_train1, s_train0, s_train1, adv_optimizer, sensitive_loss_fn, threshold)\n",
    "    model.eval()\n",
    "    writer_epoch += 1\n",
    "    model.train()\n",
    "    model.eval()\n",
    "with torch.no_grad():\n",
    "    y_test_pred = model(X_test0, X_test1)\n",
    "    test_loss = loss_fn(y_test, y_test_pred)\n",
    "    test_acc = calc_accuracy(y_test_pred, y_test)\n",
    "    di_test_score = disparate_impact(y_test_pred, s_test0, s_test1)\n",
    "    print(f'Test Loss: {test_loss.item():.4f}\\t Test Accuracy: {test_acc.item():.4f}\\t DI: {di_test_score:.4f}')\n",
    "    auc_test = auc_estimator(y_test_pred, y_test)\n",
    "    ndcg_test = nDCG_cls(model, X_test0, X_test1, y_test, esti=False)\n",
    "    gpa_test = group_pairwise_accuracy(y_test_pred, y_test, s_test0)\n",
    "    sensitive_pred0, sensitive_pred1 = model.forward_2(X_test0, X_test1)\n",
    "    sensitive_loss = sensitive_loss_fn(torch.cat((sensitive_pred0, sensitive_pred1), dim=0), torch.cat((s_test0, s_test1), dim=0))\n",
    "    rnd_arr = []\n",
    "    for i in range(100):\n",
    "        rnd_arr.append(calc_rnd(model, X_test0, X_test1, s_test0, s_test1))\n",
    "    rnd_test = np.mean(rnd_arr)\n",
    "    with open(f'{PATH}/results_base_ranker.txt', 'a+') as file:\n",
    "        file.write(f'{auc_test},{test_acc.item()},{rnd_test},{ndcg_test},{gpa_test},{sensitive_loss},{str(schedule).replace(\",\",\";\")}\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
