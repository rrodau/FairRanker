{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from FairRanking.helpers import nDCG_cls, disparate_impact, calc_accuracy, calc_sens_loss, rND_torch, auc_estimator\n",
    "from FairRanking.datasets.adult import Adult\n",
    "from FairRanking.datasets.law import Law\n",
    "from FairRanking.datasets.compas import Compas\n",
    "from FairRanking.datasets.wiki import Wiki\n",
    "from FairRanking.models.BaseDirectRanker import convert_data_to_tensors\n",
    "from FairRanking.models.DirectRankerAdv import DirectRankerAdv\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from FairRanking.writer import Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = Law('race')\n",
    "data = Adult()\n",
    "#data = Compas()\n",
    "#data = Wiki()\n",
    "(X_train, s_train, y_train), (X_val, s_val, y_val), (X_test, s_test, y_test) = data.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robert/Desktop/Bachelor/FairRanker/FairRanking/models/BaseDirectRanker.py:205: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_9d63z49rj_/croot/pytorch_1681837279022/work/torch/csrc/utils/tensor_new.cpp:233.)\n",
      "  x0 = torch.tensor(x0, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "X_train0, X_train1, s_train0, s_train1, y_train, X_val0, X_val1, s_val0, s_val1, y_val, X_test0, X_test1, s_test0, s_test1, y_test = convert_data_to_tensors(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "def calc_rnd(model, X0, X1, s0, s1):\n",
    "    zero_documents = torch.zeros(size=(X0.shape[0]+X1.shape[0], X0.shape[1]))\n",
    "    X_test_combined = torch.cat((X0, X1), dim=0)\n",
    "    shuffled = X_test_combined[torch.randperm(X_test_combined.size(0))]\n",
    "    s_test_combined = torch.cat((s0, s1), dim=0)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test_combined, shuffled)\n",
    "        s_test_combined = torch.argmax(s_test_combined, dim=1)\n",
    "        return rND_torch(predictions, s_test_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_phase(model, X_train0, X_train1, y_train, optimizer, loss_fn):\n",
    "    y_pred_train = model(X_train0, X_train1)\n",
    "    main_loss = loss_fn(y_train, y_pred_train)\n",
    "    main_loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return main_loss, y_pred_train\n",
    "\n",
    "\n",
    "def second_phase(model, X_train0, X_train1, s_train0, s_train1, optimizer, loss_fn, loss_threshold):\n",
    "    sensitive_pred0, sensitive_pred1 = model.forward_2(X_train0, X_train1)\n",
    "    sensitive_loss = loss_fn(torch.cat((sensitive_pred0, sensitive_pred1), dim=0), torch.cat((s_train0, s_train1), dim=0))\n",
    "    #sensitive_loss = loss_fn(sensitive_pred0, s_train0)\n",
    "    sensitive_loss.backward()\n",
    "    #reversed_gradients = [p.grad * -1.0 for p in model.parameters() if p.grad is not None]\n",
    "    for p in model.named_parameters():\n",
    "        if 'debias' in p[0]:\n",
    "            continue\n",
    "        else:\n",
    "            if p[1].grad is not None:\n",
    "                if sensitive_loss.item() < loss_threshold:\n",
    "                    p[1].grad *= -1\n",
    "                else:\n",
    "                    p[1].grad = torch.zeros_like(p[1].grad, dtype=torch.float32)\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return sensitive_loss\n",
    "\n",
    "def model_rel_evaluation(model, writer, epoch, loss_fn):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_train_pred = model(X_train0, X_train1)\n",
    "        y_val_pred = model(X_val0, X_val1)\n",
    "        di_train_score = disparate_impact(y_train_pred, s_train0, s_train1)\n",
    "        train_acc = calc_accuracy(y_pred_train, y_train)\n",
    "        val_acc = calc_accuracy(y_val_pred, y_val)\n",
    "        di_val_score = disparate_impact(y_val_pred, s_val0, s_val1)\n",
    "        auc_train = auc_estimator(y_pred_train, y_train)\n",
    "        auc_val = auc_estimator(y_val_train, y_val)\n",
    "        rnd_train = calc_rnd(model, X_train0, X_train1, s_train0, s_train1)\n",
    "        rnd_val = calc_rnd(model, X_val0, X_val1, s_val0, s_val1)\n",
    "        ndcg_train = nDCG_cls(model, X_train0, X_train1, y_train, esti=False)\n",
    "        ndcg_val = nDCG_cls(model, X_val0, X_val1, y_val, esti=False)\n",
    "        gpa_train = group_pairwise_accuracy(y_pred_train, y_train, s_train0)\n",
    "        gpa_val = group_pairwise_accuracy(y_val_pred, y_val, s_val0) \n",
    "        sensitive_losses = {'train_sensitive': sensitive_loss, 'val_sensitive': sensitive_val_loss}\n",
    "        disparate_impact_losses = {'train_di': di_train_score, 'val_di': di_val_score}\n",
    "        epoch_accuracies = {'train_acc': train_acc, 'val_acc': val_acc}\n",
    "        epoch_auc = {'train_auc': auc_train, 'val_auc': auc_val}\n",
    "        epoch_rnd = {'train_rnd': rnd_train, 'val_rnd': rnd_val}\n",
    "        epoch_ndcg = {'train_ndcg': ndcg_train, 'val_ndcg': ndcg_val}\n",
    "        epoch_gpa = {'train_gpa': gpa_train, 'val_ndcg': gpa_val}\n",
    "        input = {\n",
    "            'Accuracy': [epoch_accuracies, epoch],\n",
    "            'Sensitive Loss': [sensitive_losses, epoch],\n",
    "            'Disparate Impact': [disparate_impact_losses, epoch],\n",
    "            'AUC': [epoch_auc, epoch],\n",
    "            'rND': [epoch_rnd, epoch],\n",
    "            'NDCG': [epoch_ndcg, epoch],\n",
    "            'GPA': [epoch_gpa, epoch]\n",
    "            }\n",
    "        return train_acc, di_train_score, val_acc, di_val_score\n",
    "\n",
    "def model_evaluation(model, sensitive_loss, writer, writer_epoch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_train_pred = model(X_train0, X_train1)\n",
    "        y_val_pred = model(X_val0, X_val1)\n",
    "        y_pred_train = model(X_train0, X_train1)\n",
    "        di_train_score = disparate_impact(y_pred_train, s_train0, s_train1)\n",
    "        di_val_score = disparate_impact(y_val_pred, s_val0, s_val1)\n",
    "        sensitive_val_pred0, sensitive_val_pred1 = model.forward_2(X_val0, X_val1)\n",
    "        sensitive_val_loss = calc_sens_loss(sensitive_val_pred0, sensitive_val_pred1, s_val0, s_val1, gamma=1.0)\n",
    "        train_acc = calc_accuracy(y_pred_train, y_train)\n",
    "        val_acc = calc_accuracy(y_val_pred, y_val)\n",
    "        auc_train = auc_estimator(y_pred_train, y_train)\n",
    "        auc_val = auc_estimator(y_val_pred, y_val)\n",
    "        rnd_train = calc_rnd(model, X_train0, X_train1, s_train0, s_train1)\n",
    "        rnd_val = calc_rnd(model, X_val0, X_val1, s_val0, s_val1)\n",
    "        ndcg_train = nDCG_cls(model, X_train0, X_train1, y_train, esti=False)\n",
    "        ndcg_val = nDCG_cls(model, X_val0, X_val1, y_val, esti=False)\n",
    "        gpa_train = group_pairwise_accuracy(y_pred_train, y_train, s_train0)\n",
    "        gpa_val = group_pairwise_accuracy(y_val_pred, y_val, s_val0) \n",
    "        sensitive_losses = {'train_sensitive': sensitive_loss, 'val_sensitive': sensitive_val_loss}\n",
    "        disparate_impact_losses = {'train_di': di_train_score, 'val_di': di_val_score}\n",
    "        epoch_accuracies = {'train_acc': train_acc, 'val_acc': val_acc}\n",
    "        epoch_auc = {'train_auc': auc_train, 'val_auc': auc_val}\n",
    "        epoch_rnd = {'train_rnd': rnd_train, 'val_rnd': rnd_val}\n",
    "        epoch_ndcg = {'train_ndcg': ndcg_train, 'val_ndcg': ndcg_val}\n",
    "        epoch_gpa = {'train_gpa': gpa_train, 'val_ndcg': gpa_val}\n",
    "        input = {\n",
    "            'Accuracy': [epoch_accuracies, epoch],\n",
    "            'Sensitive Loss': [sensitive_losses, epoch],\n",
    "            'Disparate Impact': [disparate_impact_losses, epoch],\n",
    "            'AUC': [epoch_auc, epoch],\n",
    "            'rND': [epoch_rnd, epoch],\n",
    "            'NDCG': [epoch_ndcg, epoch],\n",
    "            'GPA': [epoch_gpa, epoch]\n",
    "            }\n",
    "        writer.write(**input)\n",
    "        return train_acc, di_train_score, val_acc, sensitive_val_loss, di_val_score\n",
    "\n",
    "\n",
    "def only_adversarial(model, X_train0, X_train1, s_train0, s_train1, optimizer, loss_fn):\n",
    "    sensitive_pred0, sensitive_pred1 = model.forward_2(X_train0, X_train1)\n",
    "    sensitive_loss = loss_fn(torch.cat((sensitive_pred0, sensitive_pred1), dim=0), torch.cat((s_train0, s_train1), dim=0))\n",
    "    sensitive_loss.backward()\n",
    "    for p in model.named_parameters():\n",
    "        if 'debias' in p[0]:\n",
    "            #print(p[1])\n",
    "            continue\n",
    "        else:\n",
    "            if p[1].grad is not None:\n",
    "                p[1].grad = torch.zeros_like(p[1].grad, dtype=torch.float32)\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return sensitive_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Created SummaryWriter saving to runs/adult/2023-12-29/Schedule_1/run_1/directRankerADV/disparateImpactNormalWeightInitial\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_val_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/robert/Desktop/Bachelor/FairRanker/experimentDirectRankerAdv.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robert/Desktop/Bachelor/FairRanker/experimentDirectRankerAdv.ipynb#X40sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robert/Desktop/Bachelor/FairRanker/experimentDirectRankerAdv.ipynb#X40sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m train_loss \u001b[39m=\u001b[39m main_loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robert/Desktop/Bachelor/FairRanker/experimentDirectRankerAdv.ipynb#X40sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m train_acc, di_train_score, val_acc, di_val_score \u001b[39m=\u001b[39m \\\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/robert/Desktop/Bachelor/FairRanker/experimentDirectRankerAdv.ipynb#X40sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     model_rel_evaluation(model, writer, epoch, loss_fn)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robert/Desktop/Bachelor/FairRanker/experimentDirectRankerAdv.ipynb#X40sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robert/Desktop/Bachelor/FairRanker/experimentDirectRankerAdv.ipynb#X40sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m writer_epoch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;32m/Users/robert/Desktop/Bachelor/FairRanker/experimentDirectRankerAdv.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robert/Desktop/Bachelor/FairRanker/experimentDirectRankerAdv.ipynb#X40sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m di_val_score \u001b[39m=\u001b[39m disparate_impact(y_val_pred, s_val0, s_val1)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robert/Desktop/Bachelor/FairRanker/experimentDirectRankerAdv.ipynb#X40sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m auc_train \u001b[39m=\u001b[39m auc_estimator(y_pred_train, y_train)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/robert/Desktop/Bachelor/FairRanker/experimentDirectRankerAdv.ipynb#X40sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m auc_val \u001b[39m=\u001b[39m auc_estimator(y_val_train, y_val)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robert/Desktop/Bachelor/FairRanker/experimentDirectRankerAdv.ipynb#X40sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m rnd_train \u001b[39m=\u001b[39m calc_rnd(model, X_train0, X_train1, s_train0, s_train1)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robert/Desktop/Bachelor/FairRanker/experimentDirectRankerAdv.ipynb#X40sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m rnd_val \u001b[39m=\u001b[39m calc_rnd(model, X_val0, X_val1, s_val0, s_val1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_val_train' is not defined"
     ]
    }
   ],
   "source": [
    "scheduler_run = 'Schedule_1'\n",
    "\n",
    "# rnd score in the original data\n",
    "y_test_full = torch.cat((y_test, (-1)*y_test), dim=0)\n",
    "s_test_full = torch.cat((s_test0, s_test1), dim=0)\n",
    "base_rnd = rND_torch(y_test_full, torch.argmax(s_test_full, dim=1))\n",
    "\n",
    "hidden_layers = [64, 32, 16]\n",
    "debias_layers = [128, 64, 32, 16]\n",
    "threshold = 0.35\n",
    "main_lr = 0.001\n",
    "adv_lr = 0.1\n",
    "k = 1\n",
    "schedules = [[i,j] for j in range(1,4) for i in range(1,4)]\n",
    "for schedule in schedules:\n",
    "    torch.manual_seed(42)\n",
    "    model = DirectRankerAdv(num_features=X_train0.shape[1],\n",
    "                     kernel_initializer=nn.init.normal_,\n",
    "                     hidden_layers=hidden_layers,\n",
    "                     bias_layers=debias_layers,\n",
    "             )\n",
    "    n_epochs = int(1000 / schedule[0])\n",
    "    writer = Writer(hidden_layers,\n",
    "                    schedule,\n",
    "                    scheduler_run,\n",
    "                    'adult',\n",
    "                    f'run_{k}',\n",
    "                    'directRankerADV',\n",
    "                    'disparateImpactNormalWeightInitial'\n",
    "                    )\n",
    "    writer.writer.add_text(tag='Debias Layers', text_string=str(debias_layers))\n",
    "    writer.writer.add_text(tag='Main lr', text_string=str(main_lr))\n",
    "    writer.writer.add_text(tag='Adv lr', text_string=str(adv_lr))\n",
    "    writer.writer.add_text(tag='threshold', text_string=str(threshold))\n",
    "    lr_decay = 0.944\n",
    "    optimizer = optim.Adam(model.parameters(), lr=main_lr)\n",
    "    adv_optimizer = optim.Adam(model.parameters(), lr=adv_lr)\n",
    "    opt_scheduler = StepLR(optimizer, step_size=500, gamma=lr_decay)\n",
    "    adv_scheduler = StepLR(adv_optimizer, step_size=500, gamma=lr_decay)\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "    sensitive_loss_fn = nn.BCEWithLogitsLoss()\n",
    "    writer_epoch = 0 \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for i in range(schedule[0]):\n",
    "            main_loss, y_pred_train = first_phase(model, X_train0, X_train1, y_train, optimizer, loss_fn)\n",
    "            model.eval()\n",
    "            train_loss = main_loss.item()\n",
    "            train_acc, di_train_score, val_acc, di_val_score = \\\n",
    "                model_rel_evaluation(model, writer, epoch, loss_fn)\n",
    "            model.train()\n",
    "            writer_epoch += 1\n",
    "        for i in range(schedule[1]):\n",
    "            sensitive_loss = second_phase(model, X_train0, X_train1, s_train0, s_train1, adv_optimizer, sensitive_loss_fn, threshold)\n",
    "            with open('sensitive_loss.txt', 'a') as file:\n",
    "                file.write(f\"{writer_epoch}    {sensitive_loss.item():.4f}\\n\")\n",
    "            model.eval()\n",
    "            train_acc, di_train_score, val_acc, sensitive_val_loss, di_val_score = \\\n",
    "                model_evaluation(model, sensitive_loss, writer, writer_epoch)\n",
    "            writer_epoch += 1\n",
    "            model.train()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_test_pred = model(X_test0, X_test1)\n",
    "        test_loss = loss_fn(y_test, y_test_pred)\n",
    "        test_acc = calc_accuracy(y_test_pred, y_test)\n",
    "        di_test_score = disparate_impact(y_test_pred, s_test0, s_test1)\n",
    "        print(f'Test Loss: {test_loss.item():.4f}\\t Test Accuracy: {test_acc.item():.4f}\\t DI: {di_test_score:.4f}')\n",
    "        auc_test = auc_estimator(y_test_pred, y_test)\n",
    "        rnd_test = calc_rnd(model, X_test0, X_test1, s_test0, s_test1)\n",
    "        ndcg_test = nDCG_cls(model, X_test0, X_test1, y_test, esti=False)\n",
    "        gpa_test = group_pairwise_accuracy(y_test_pred, y_test, s_test0)\n",
    "        sensitive_pred0, sensitive_pred1 = model.forward_2(X_test0, X_test1)\n",
    "        sensitive_loss = loss_fn(torch.cat((sensitive_pred0, sensitive_pred1), dim=0), torch.cat((s_test0, s_test1), dim=0))\n",
    "        writer.writer.add_text(tag='Test Accuracy', text_string=str(acc_test.item()))\n",
    "        writer.writer.add_text(tag='Test AUC', text_string=str(auc_test))\n",
    "        writer.writer.add_text(tag='Test NDCG', text_string=str(ndcg_test))\n",
    "        writer.writer.add_text(tag='Test GPA', text_string=str(gpa_test))\n",
    "        writer.writer.add_text(tag='Test rND', text_string=str(rnd_test))\n",
    "        writer.writer.add_text(tag='Test Sensitive Loss', text_string=str(sensitive_loss.item()))\n",
    "        with open('test_results/results_real_schedule_1.txt', 'a') as file:\n",
    "            file.write(f'{auc_test},{test_acc.item()},{rnd_test},{ndcg_test},{gpa_test},{sensitive_loss},{str(schedule).replace(\",\",\";\")}\\n')\n",
    "    writer_epoch = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        for i in range(schedule[1]):\n",
    "            model_train()\n",
    "            sensitive_loss = only_adversarial(model, X_train0, X_train1, s_train0, s_train1, adv_optimizer, sensitive_loss_fn)\n",
    "            writer.write({'Train Sensitive Loss': sensitive_loss})\n",
    "            writer_epoch += 1\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sensitive_pred0, sensitive_pred1 = model.forward_2(X_test0, X_test1)\n",
    "        test_sensitive_loss = loss_fn(torch.cat((sensitive_pred0, sensitive_pred1), dim=0), torch.cat((s_test0, s_test1), dim=0))\n",
    "        writer.writer.add_text(tag='Sensitive Loss (Seperate Training)', text_string=str(test_sensitive_loss))\n",
    "    del writer\n",
    "    print('Finished')\n",
    "print(\"ALL FINISHED!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
