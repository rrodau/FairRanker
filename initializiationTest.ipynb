{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from FairRanking.helpers import nDCG_cls, disparate_impact, calc_accuracy, calc_sens_loss, rND_torch, auc_estimator\n",
    "from FairRanking.datasets.adult import Adult\n",
    "from FairRanking.datasets.law import Law\n",
    "from FairRanking.datasets.compas import Compas\n",
    "from FairRanking.datasets.wiki import Wiki\n",
    "from FairRanking.models.BaseDirectRanker import build_pairs1, convert_data_to_tensors\n",
    "from FairRanking.models.DirectRankerAdv import DirectRankerAdv\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = Law('race')\n",
    "data = Adult()\n",
    "#data = Compas()\n",
    "#data = Wiki()\n",
    "(X_train, s_train, y_train), (X_val, s_val, y_val), (X_test, s_test, y_test) = data.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robert/Desktop/Bachelor/FairRanker/FairRanking/models/BaseDirectRanker.py:205: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_9d63z49rj_/croot/pytorch_1681837279022/work/torch/csrc/utils/tensor_new.cpp:233.)\n",
      "  x0 = torch.tensor(x0, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "X_train0, X_train1, s_train0, s_train1, y_train, X_val0, X_val1, s_val0, s_val1, y_val, X_test0, X_test1, s_test0, s_test1, y_test = convert_data_to_tensors(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_phase(model, X_train0, X_train1, y_train, optimizer, loss_fn):\n",
    "    y_pred_train = model(X_train0, X_train1)\n",
    "    main_loss = loss_fn(y_train, y_pred_train)\n",
    "    main_loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return main_loss, y_pred_train\n",
    "\n",
    "\n",
    "def second_phase(model, X_train0, X_train1, s_train0, s_train1, optimizer, loss_fn):\n",
    "    sensitive_pred0, sensitive_pred1 = model.forward_2(X_train0, X_train1)\n",
    "    sensitive_loss = loss_fn(torch.cat((sensitive_pred0, sensitive_pred1), dim=0), torch.cat((s_train0, s_train1), dim=0))\n",
    "    #sensitive_loss = loss_fn(sensitive_pred0, s_train0)\n",
    "    sensitive_loss.backward()\n",
    "    #reversed_gradients = [p.grad * -1.0 for p in model.parameters() if p.grad is not None]\n",
    "    for p in model.named_parameters():\n",
    "        if 'debias' in p[0]:\n",
    "            #print(p[1])\n",
    "            continue\n",
    "        else:\n",
    "            if p[1].grad is not None:\n",
    "                if sensitive_loss.item() < 0.41:\n",
    "                    p[1].grad = torch.zeros_like(p[1].grad, dtype=torch.float32)\n",
    "                    continue\n",
    "                    p[1].grad *= -1\n",
    "                else:\n",
    "                    p[1].grad = torch.zeros_like(p[1].grad, dtype=torch.float32)\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return sensitive_loss\n",
    "\n",
    "def model_rel_evaluation(model, X_val0, X_val1, s_train0, s_train1, main_loss, loss_fn, y_pred_train, y_train):\n",
    "    di_train_score = disparate_impact(y_pred_train, s_train0, s_train1)\n",
    "    y_val_pred = model(X_val0, X_val1)\n",
    "    val_loss = loss_fn(y_val, y_val_pred)\n",
    "    train_acc = calc_accuracy(y_pred_train, y_train)\n",
    "    val_acc = calc_accuracy(y_val_pred, y_val)\n",
    "    di_val_score = disparate_impact(y_val_pred, s_val0, s_val1)\n",
    "    return main_loss.item(), train_acc, di_train_score, val_loss.item(), val_acc, di_val_score\n",
    "\n",
    "def model_evaluation(model, X_val0, X_val1, s_val0, s_val1, y_train, y_val, main_loss, loss_fn, y_pred_train):\n",
    "    y_val_pred = model(X_val0, X_val1)\n",
    "    di_train_score = disparate_impact(y_pred_train, s_train0, s_train1)\n",
    "    di_val_score = disparate_impact(y_val_pred, s_val0, s_val1)\n",
    "    sensitive_val_pred0, sensitive_val_pred1 = model.forward_2(X_val0, X_val1)\n",
    "    val_loss = loss_fn(y_val, y_val_pred)\n",
    "    sensitive_val_loss = calc_sens_loss(sensitive_val_pred0, sensitive_val_pred1, s_val0, s_val1, gamma=1.0)\n",
    "    train_acc = calc_accuracy(y_pred_train, y_train)\n",
    "    val_acc = calc_accuracy(y_val_pred, y_val)\n",
    "    return main_loss.item(), train_acc, di_train_score, val_loss.item(), val_acc, sensitive_val_loss, di_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/1000 Train Loss: 0.1481  Train Accuracy: 0.9570  Train DI: 15.4622\t Val Accuracy: 0.4979  Val DI 7.8558\n",
      "100/1000 Train Loss: 0.1481  Train Accuracy: 0.9570  Train DI: 15.4622\t Val Accuracy: 0.4979  Val DI 7.8558\n",
      "150/1000 Train Loss: 0.1481  Train Accuracy: 0.9570  Train DI: 15.4622\t Val Accuracy: 0.4979  Val DI 7.8558\n",
      "200/1000 Train Loss: 0.1481  Train Accuracy: 0.9570  Train DI: 15.4622\t Val Accuracy: 0.4979  Val DI 7.8558\n",
      "250/1000 Train Loss: 0.1481  Train Accuracy: 0.9570  Train DI: 15.4622\t Val Accuracy: 0.4979  Val DI 7.8558\n",
      "300/1000 Train Loss: 0.1481  Train Accuracy: 0.9570  Train DI: 15.4622\t Val Accuracy: 0.4979  Val DI 7.8558\n",
      "350/1000 Train Loss: 0.1481  Train Accuracy: 0.9570  Train DI: 15.4622\t Val Accuracy: 0.4979  Val DI 7.8558\n",
      "400/1000 Train Loss: 0.1481  Train Accuracy: 0.9570  Train DI: 15.4622\t Val Accuracy: 0.4979  Val DI 7.8558\n",
      "450/1000 Train Loss: 0.1481  Train Accuracy: 0.9570  Train DI: 15.4622\t Val Accuracy: 0.4979  Val DI 7.8558\n",
      "500/1000 Train Loss: 0.1481  Train Accuracy: 0.9570  Train DI: 15.4622\t Val Accuracy: 0.4979  Val DI 7.8558\n",
      "550/1000 Train Loss: 0.1481  Train Accuracy: 0.9570  Train DI: 15.4622\t Val Accuracy: 0.4979  Val DI 7.8558\n",
      "600/1000 Train Loss: 0.1481  Train Accuracy: 0.9570  Train DI: 15.4622\t Val Accuracy: 0.4979  Val DI 7.8558\n",
      "650/1000 Train Loss: 0.1481  Train Accuracy: 0.9570  Train DI: 15.4622\t Val Accuracy: 0.4979  Val DI 7.8558\n",
      "700/1000 Train Loss: 0.1481  Train Accuracy: 0.9570  Train DI: 15.4622\t Val Accuracy: 0.4979  Val DI 7.8558\n",
      "750/1000 Train Loss: 0.1481  Train Accuracy: 0.9570  Train DI: 15.4622\t Val Accuracy: 0.4979  Val DI 7.8558\n",
      "800/1000 Train Loss: 0.1481  Train Accuracy: 0.9570  Train DI: 15.4622\t Val Accuracy: 0.4979  Val DI 7.8558\n",
      "850/1000 Train Loss: 0.1481  Train Accuracy: 0.9570  Train DI: 15.4622\t Val Accuracy: 0.4979  Val DI 7.8558\n",
      "900/1000 Train Loss: 0.1481  Train Accuracy: 0.9570  Train DI: 15.4622\t Val Accuracy: 0.4979  Val DI 7.8558\n",
      "950/1000 Train Loss: 0.1481  Train Accuracy: 0.9570  Train DI: 15.4622\t Val Accuracy: 0.4979  Val DI 7.8558\n",
      "1000/1000 Train Loss: 0.1481  Train Accuracy: 0.9570  Train DI: 15.4622\t Val Accuracy: 0.4979  Val DI 7.8558\n",
      "Test Loss: 1.2074\t Test Accuracy: 0.5178\t DI: 10.0124\n",
      "Finished\n",
      "ALL FINISHED!\n"
     ]
    }
   ],
   "source": [
    "hidden_layers = [64, 32, 16]\n",
    "#debias_layers = [32, 16, 2]\n",
    "schedules = [[i,j] for j in range(1,8) for i in range(1,8)]\n",
    "schedules = [[0,1]]\n",
    "for schedule in schedules:\n",
    "    torch.manual_seed(42)\n",
    "    model = DirectRankerAdv(num_features=X_train0.shape[1],\n",
    "                     kernel_initializer=nn.init.normal_,\n",
    "                     hidden_layers=hidden_layers,\n",
    "                     bias_layers=[128, 64, 32, 16],\n",
    "             )\n",
    "    #n_epochs = int(1000 / schedule[0])\n",
    "    n_epochs = 1000\n",
    "    lr = 0.001\n",
    "    lr_decay = 0.944\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    adv_optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "    opt_scheduler = StepLR(optimizer, step_size=500, gamma=lr_decay)\n",
    "    adv_scheduler = StepLR(adv_optimizer, step_size=500, gamma=lr_decay)\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "    sensitive_loss_fn = nn.BCEWithLogitsLoss()\n",
    "    writer_epoch = 0 \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for i in range(schedule[0]):\n",
    "            main_loss, y_pred_train = first_phase(model, X_train0, X_train1, y_train, optimizer, loss_fn)\n",
    "            model.eval()\n",
    "            train_loss, train_acc, di_train_score, val_loss, val_acc, di_val_score = model_rel_evaluation(model, X_val0, X_val1, s_train0, s_train1, main_loss, loss_fn, y_pred_train, y_train)\n",
    "            model.train()\n",
    "            writer_epoch += 1\n",
    "        for i in range(schedule[1]):\n",
    "            sensitive_loss = second_phase(model, X_train0, X_train1, s_train0, s_train1, adv_optimizer, sensitive_loss_fn)\n",
    "            with open('sensitive_loss.txt', 'a') as file:\n",
    "                file.write(f\"{writer_epoch}    {sensitive_loss.item():.4f}\\n\")\n",
    "            model.eval()\n",
    "            train_loss, train_acc, di_train_score, val_loss, val_acc, sensitive_val_loss, di_val_score = model_evaluation(model, X_val0, X_val1, s_val0, s_val1, y_train, y_val, main_loss, loss_fn, y_pred_train)\n",
    "            writer_epoch += 1\n",
    "            model.train()\n",
    "        #opt_scheduler.step()\n",
    "        #adv_scheduler.step()\n",
    "        if writer_epoch % 50 == 0:\n",
    "            print(f\"{writer_epoch}/{n_epochs*schedule[0]+n_epochs*schedule[1]} Train Loss: {main_loss.item():.4f}  Train Accuracy: {train_acc:.4f}  Train DI: {di_train_score:.4f}\\t Val Accuracy: {val_acc:.4f}  Val DI {di_val_score:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    y_test_pred = model(X_test0, X_test1)\n",
    "    test_loss = loss_fn(y_test, y_test_pred)\n",
    "    test_acc = calc_accuracy(y_test_pred, y_test)\n",
    "    di_test_score = disparate_impact(y_test_pred, s_test0, s_test1)\n",
    "    print(f'Test Loss: {test_loss.item():.4f}\\t Test Accuracy: {test_acc.item():.4f}\\t DI: {di_test_score:.4f}')\n",
    "    print('Finished')\n",
    "print(\"ALL FINISHED!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nDCG_cls(model, X_test0, X_test1, y_test, esti=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "def calc_rnd(model, X0, X1, s0, s1):\n",
    "    zero_documents = torch.zeros(size=(X0.shape[0]+X1.shape[0], X0.shape[1]))\n",
    "    X_test_combined = torch.cat((X0, X1), dim=0)\n",
    "    shuffled = X_test_combined[torch.randperm(X_test_combined.size(0))]\n",
    "    s_test_combined = torch.cat((s0, s1), dim=0)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test_combined, shuffled)\n",
    "        s_test_combined = torch.argmax(s_test_combined, dim=1)\n",
    "        return rND_torch(predictions, s_test_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base rND: 0.2082092918516293\n",
      "Test rND: 0.08992022326871894\n"
     ]
    }
   ],
   "source": [
    "# rnd score in the original data\n",
    "y_test_full = torch.cat((y_test, (-1)*y_test), dim=0)\n",
    "s_test_full = torch.cat((s_test0, s_test1), dim=0)\n",
    "base_rnd = rND_torch(y_test_full, torch.argmax(s_test_full, dim=1))\n",
    "\n",
    "test_rnd = calc_rnd(model, X_test0, X_test1, s_test0, s_test1)\n",
    "print(f\"Base rND: {base_rnd}\")\n",
    "print(f\"Test rND: {test_rnd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5178230404853821\n",
      "AUC: 0.5232743297214268\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_test_test = torch.zeros_like(y_test)\n",
    "with torch.no_grad():\n",
    "    pred = model(X_test0, X_test1)\n",
    "    print(f\"Accuracy: {calc_accuracy(pred, y_test)}\")\n",
    "    print(f\"AUC: {auc_estimator(pred, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(calc_sens_loss(s_train0, s_train1, s_train0, s_train1))\n",
    "bce_loss_fn = nn.BCELoss()\n",
    "bce_loss_fn(torch.cat((s_train0, s_train1), dim=0), torch.cat((s_train0, s_train1), dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5588739514350891\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_bce_fn = nn.BCEWithLogitsLoss()\n",
    "    sensitive_pred0, sensitive_pred1 = model.forward_2(X_val0, X_val1)\n",
    "    sensitive_loss = test_bce_fn(torch.cat((sensitive_pred0, sensitive_pred1), dim=0), torch.cat((s_val0, s_val1), dim=0))\n",
    "    print(sensitive_loss.item())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'trained_model/testModel1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layers = [64, 32, 16]\n",
    "debias_layers = [128, 64, 32, 16]\n",
    "model = DirectRankerAdv(num_features=X_train0.shape[1],\n",
    "                    kernel_initializer=nn.init.normal_,\n",
    "                    hidden_layers=hidden_layers,\n",
    "                    bias_layers=debias_layers)\n",
    "model.load_state_dict(torch.load('trained_model/testModel1.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07545982338650775\n"
     ]
    }
   ],
   "source": [
    "from FairRanking.helpers import group_pairwise_accuracy\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(X_test0, X_test1)\n",
    "    print(group_pairwise_accuracy(pred, y_test, s_test0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
